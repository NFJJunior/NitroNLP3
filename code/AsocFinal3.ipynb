{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":22494,"status":"ok","timestamp":1711836068427,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"5sI4ZQzZKXy5"},"outputs":[],"source":["### Imports\n","\n","import os\n","import torch\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import RandomSampler, Sampler\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from typing import Iterator, Callable, List, Tuple\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","import random\n","import collections"]},{"cell_type":"markdown","metadata":{"id":"vWNd27_TxZiV"},"source":["# PENTRU MODEL - TARAM TEODORA"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711836068427,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"So2_dcqExesC"},"outputs":[],"source":["bert_used = 'readerbench/RoBERT-large'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711836068427,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"r-teL4_Rx2jZ"},"outputs":[],"source":["BATCH_SIZE = 128"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711836068427,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"usxlQGn2x85A"},"outputs":[],"source":["USED_TYPE = 1\n","#### 0 for TITLE\n","#### 1 for CONTENT"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711836068428,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"HwdvcAjNyDOe"},"outputs":[],"source":["MAX_TITLE_LEN = 90\n","MAX_CONTENT_LEN = 2048"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711836068428,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"p-FVcgq0w_Ft","outputId":"9000d71b-fa9c-4f20-9563-be3684036063"},"outputs":[{"name":"stdout","output_type":"stream","text":["NumPy random seed set with value: 7757\n","TensorFlow random seed set with value: 7757\n","PyTorch random seed set with value: 7757\n"]}],"source":["import random\n","\n","\n","def reset_numpy_seed(seed_value=42):\n","  try:\n","    # Set NumPy random seed\n","    import numpy as np\n","    np.random.seed(seed_value)\n","    print(f'NumPy random seed set with value: {seed_value}')\n","  except Exception as e:\n","    print(f'NumPy random seed was not set: {e}')\n","  return\n","\n","\n","def reset_tensorflow_seed(seed_value=42):\n","  try:\n","    # Set TensorFlow random seed\n","    import tensorflow as tf\n","    success = False\n","    # Here we have 2 different ways to set the seed\n","    # depending on the version of TensorFlow\n","    try:\n","      tf.random.set_seed(seed_value)\n","      success = True\n","    except Exception as e:\n","      pass\n","    try:\n","      tf.set_random_seed(seed_value)\n","      success = True\n","    except Exception as e:\n","      pass\n","    if success:\n","      print(f'TensorFlow random seed set with value: {seed_value}')\n","    else:\n","      print(f'TensorFlow random seed was not set')\n","  except Exception as e:\n","    print(f'TensorFlow random seed was not set: {e}')\n","  return\n","\n","\n","def reset_torch_seed(seed_value=42):\n","  try:\n","    # Set PyTorch random seed\n","    import torch\n","    torch.manual_seed(seed_value)\n","    if torch.cuda.is_available():\n","      torch.cuda.manual_seed(seed_value)\n","      torch.cuda.manual_seed_all(seed_value)  # if you are using multiple GPUs\n","    print(f'PyTorch random seed set with value: {seed_value}')\n","  except Exception as e:\n","    print(f'PyTorch random seed was not set: {e}')\n","  return\n","\n","\n","def set_random_seeds(seed_value=42):\n","  # Set Python random seed\n","  random.seed(seed_value)\n","  reset_numpy_seed(seed_value)\n","  reset_tensorflow_seed(seed_value)\n","  reset_torch_seed(seed_value)\n","  return\n","\n","\n","if __name__ == '__main__':\n","  # Set the desired seed value\n","  seed = 7757\n","\n","  # Set random seeds\n","  set_random_seeds(seed)"]},{"cell_type":"markdown","metadata":{"id":"MNKgUmQAyeEk"},"source":["# GATA TARAM"]},{"cell_type":"markdown","metadata":{"id":"6bL419nZAF8Y"},"source":["# Taram Iancu"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1008,"status":"ok","timestamp":1711836069432,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"QKnTPXUeAMeS","outputId":"54c0aab3-994b-4117-f27a-65513d74e8ec"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import nltk\n","from nltk.stem.snowball import stopwords\n","nltk.download('stopwords')\n","import re\n","\n","def preprocess(text):\n","    if type(text) is str:\n","        # Remove words with more than half numbers\n","        # text = re.sub(r'\\b(?=\\w\\d)\\w\\d\\w\\b', ' ', text)\n","\n","        # Correct badly hyphened words, badly hyphened measure units, and badly formatted numbers.\n","        text = re.sub(r'(?\u003c=\\S)-(?=\\S)', '', text) # remove bad hyphens\n","        text = re.sub(r'(\\d+)(\\s)(%|°|€|¥|£|$|cm|mm|m|km|in|ft|yd|mi|g|kg|lb|oz)(?!\\w)', r'\\1\\3', text) # correct badly hyphened measure units\n","        text = re.sub(r'(\\d),(\\d)', r'\\1.\\2', text) # correct badly formatted numbers\n","\n","        # URLs\n","        text = re.sub(r'\\(?\\s*https?\\S*\\s*\\)?', r' ', text)\n","\n","        # Sites\n","        text = re.sub(r'\\s*\\s', r' ', text)\n","\n","        text = re.sub(r'\\(?\\s*@\\S*\\s*\\)?', r' Persoană ', text)\n","\n","        # text = re.sub(r'@\\w+', '', text)\n","        # text = text.replace('@', '') # remove soft hyphens\n","        # text = text.replace('\\xad', '') # remove soft hyphens\n","\n","        # Normalize dashes and other characters.\n","        text = text.replace('–', '-') # normalize dashes\n","        text = text.replace('—', '-') # normalize dashes\n","        text = text.replace('−', '-') # normalize dashes\n","        text = text.replace('‑', '-') # normalize dashes\n","        text = text.replace('“', '\"') # normalize quotation marks\n","        text = text.replace('”', '\"') # normalize quotation marks\n","        text = text.replace('„', '\"') # normalize quotation marks\n","        text = re.sub(r'(?\u003c=[.,;:?!])(?=[^\\s])', r' ', text)\n","\n","        # Reduce multiple spaces.\n","        text = re.sub(' +',' ',text)\n","\n","        if text[0]==' ':\n","            text = text[1:]\n","\n","        # diacritice = 'âăîțșÎĂȚȘÂ'\n","        # normale = 'aaitsiatsa'\n","        # for i in range(len(diacritice)):\n","        #     text = text.replace(diacritice[i], normale[i])\n","\n","        return ' '.join([x for x in text.strip().split() if x not in stopwords.words('romanian')]).strip()\n","\n","        return text\n","\n","# numere de telefon\n","# hastaguri\n","# pic.twiter\n","# ++, --\n","#\n"]},{"cell_type":"markdown","metadata":{"id":"vpGaffJ6AJgC"},"source":["# Gata Taram Iancu"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33231,"status":"ok","timestamp":1711836102660,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"jjOBsPGaLlfy","outputId":"50102a79-4dd8-4094-887e-87fdc260a0f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8293,"status":"ok","timestamp":1711836110951,"user":{"displayName":"Iancu Ivasciuc","userId":"11132101042389010096"},"user_tz":-120},"id":"ymgbQxvc5UJO"},"outputs":[],"source":["read_data = pd.read_csv(\n","    \"/content/drive/MyDrive/NLP3/train.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ys4StEsYAjUp"},"outputs":[],"source":["read_data['title'] = read_data['title'].apply(lambda text: preprocess(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UY11bKPycvTn"},"outputs":[{"name":"stdout","output_type":"stream","text":["(25307, 4)\n","(45268, 4)\n"]}],"source":["\n","print(read_data[read_data['class'] == False].shape)\n","print(read_data[read_data['class'] == True].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z7B1-aDBdYDJ"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\"Gândurile Cristoiu\". Interviu generalul (r) Silviu Predoiu, exSIE: Spionajul patriotism meserie murdară'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["read_data[\"title\"][41508]"]},{"cell_type":"markdown","metadata":{"id":"WPpVMBObht7I"},"source":["Pregatirea datelor:\n","- Scoatem spatii multiple\n","- Url, email?\n","- greseli gramaticale?\n","- non-utf8\n","- procente?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CVPOQkqtd4zB"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(read_data[['title', 'content']], read_data['class'], test_size=0.20)\n","\n","df_train = pd.concat([X_train, y_train], axis=1)\n","df_test = pd.concat([X_test, y_test], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"he6aJznbedas"},"outputs":[{"name":"stdout","output_type":"stream","text":["(56460, 3)\n","(14115, 3)\n"]}],"source":["print(df_train.shape)\n","print(df_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-yw0TEtMAoXf"},"outputs":[{"name":"stdout","output_type":"stream","text":["device =  cpu\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"device = \", device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TDPD6UHttUH"},"outputs":[],"source":["#Sursa: https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(bert_used)\n","\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","        self.labels = [label for label in df['class']]\n","        self.titles = [tokenizer(str(text),\n","                                 padding='max_length', max_length = MAX_TITLE_LEN, truncation=True,\n","                                return_tensors=\"pt\") for text in df['title']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_title(self, idx):\n","        # Fetch a batch of inputs\n","        return self.titles[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_title = self.get_batch_title(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_title, batch_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDcZDZNurKjW"},"outputs":[],"source":["train_dataset = Dataset(df_train) # 11 min"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5lZ6P0Y0UtU"},"outputs":[],"source":["validation_dataset = Dataset(df_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRIIOUss0XR0"},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-5Chk0X0Ync"},"outputs":[],"source":["val_dataloader = torch.utils.data.DataLoader(\n","    validation_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uq2eA5KAenh"},"outputs":[],"source":["\n","from transformers import BertModel\n","\n","class SatireClassifier(nn.Module):\n","    def __init__(self,\n","                 input_size: int,\n","                 hidden_size_1: int,\n","                 hidden_size_2: int,\n","                 hidden_size_3: int,\n","                 device: torch.device,\n","                 activation_fn: Callable,\n","                 dropout_rate: float):\n","        super(SatireClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_used)\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size_1\n","        self.hidden_size_2 = hidden_size_2\n","        self.hidden_size_3 = hidden_size_3\n","        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n","        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.hidden_layer_3 = nn.Linear(hidden_size_2, hidden_size_3)\n","        self.output_layer = nn.Linear(hidden_size_2, 1) ####### Nu fi proasta si modifica\n","        self.activation_fn = activation_fn\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.device = device\n","\n","    def forward(self, x):\n","        x = x.to(torch.float32).to(device)\n","        h1 = self.activation_fn(self.hidden_layer_1(x))\n","        h2 = self.activation_fn(self.hidden_layer_2(h1))\n","        #h3 = self.activation_fn(self.hidden_layer_3(h2))\n","        #h = self.activation_fn(self.dropout(self.hidden_layer(x)))\n","        out = self.output_layer(h2)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41wHhcpmyyVC"},"outputs":[],"source":["def get_num_features():\n","  if USED_TYPE == 0:\n","    return MAX_TITLE_LEN\n","  else: return MAX_CONTENT_LEN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSAFQf8AD-tU"},"outputs":[],"source":["model = SatireClassifier(\n","    input_size=get_num_features(),\n","    hidden_size_1=256,\n","    hidden_size_2=512,\n","    hidden_size_3=512,\n","    device=device,\n","    activation_fn=nn.ReLU(),\n","    dropout_rate=0.2\n",")\n","model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngPhCBNCEy64"},"outputs":[],"source":["for param in model. parameters ():\n","  print(param)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"espZ2L_kFuj6"},"outputs":[],"source":["loss_crt = nn.BCEWithLogitsLoss(\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78J4NKbfMWRC"},"outputs":[],"source":["pos_weight = torch.tensor([5])\n","loss_crct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8JsO9rnJhpy"},"outputs":[],"source":["loss_crt = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtgzXNkzdyYR"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr=3e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5mzs8o4GdS3"},"outputs":[],"source":["num_train_batches = len(train_dataloader)\n","num_val_batches = len(val_dataloader)\n","\n","#epoch_loss = 0.0\n","train_losses, val_losses = [], []\n","train_predictions, val_predictions = [], []\n","train_labels, val_labels = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","NUM_EPOCHS = 25\n","\n","for epoch_idx in range(NUM_EPOCHS):\n","    train_labels = []\n","    val_labels = []\n","    train_preds = []\n","    val_preds = []\n","\n","    train_epoch_loss = 0.0\n","    model.train()\n","    for batch_content, batch_labels in tqdm(train_dataloader):\n","        model.zero_grad()\n","\n","        batch_labels = batch_labels \\\n","                  .type(torch.FloatTensor) \\\n","                  .reshape((batch_labels.shape[0], 1))\n","        batch_labels = batch_labels.to(device)\n","\n","        mask = batch_content['attention_mask'].to(device)\n","        input_id = batch_content['input_ids'].squeeze(1).to(device)\n","        # feedforward\n","        out = model(input_id)\n","\n","        probabilities = torch.sigmoid(out.squeeze())  # Apply sigmoid activation\n","        batch_predictions = (probabilities \u003e= 0.5).long()\n","\n","        # compute loss\n","        loss = loss_crt(out, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_epoch_loss += loss.item()\n","        train_labels += batch_labels.to(int).tolist()\n","        train_preds += batch_predictions.tolist()\n","\n","    val_preds = []\n","    with torch.no_grad():\n","        model.eval()\n","        val_epoch_loss = 0.0\n","        for batch_content, batch_labels in tqdm(val_dataloader):\n","            batch_labels = batch_labels \\\n","                  .type(torch.FloatTensor) \\\n","                  .reshape((batch_labels.shape[0], 1))\n","            batch_labels = batch_labels.to(device)\n","\n","            mask = batch_content['attention_mask'].to(device)\n","            input_id = batch_content['input_ids'].squeeze(1).to(device)\n","            # batch_size x 10\n","            # feedforward\n","            out = model(input_id)\n","            probabilities = torch.sigmoid(out.squeeze())  # Apply sigmoid activation\n","            batch_predictions = (probabilities \u003e= 0.5).long()\n","\n","            # compute loss\n","            loss = loss_crt(out, batch_labels)\n","            val_epoch_loss += loss.item()\n","            val_labels += batch_labels.to(int).tolist()\n","            val_preds += batch_predictions.tolist()\n","\n","    train_epoch_loss /= num_train_batches\n","    val_epoch_loss /= num_val_batches\n","    train_acc = accuracy_score(train_labels, train_preds)\n","    valid_acc = accuracy_score(val_labels, val_preds)\n","\n","    train_losses.append(train_epoch_loss)\n","    val_losses.append(val_epoch_loss)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(valid_acc)\n","\n","    print(\"epoch %d, train loss=%f, val loss=%f, train acc=%f, val acc=%f\" % (\n","        epoch_idx, train_epoch_loss, val_epoch_loss, train_acc, valid_acc\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIFwCYDNhlyO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-OVYe3vKRBo"},"outputs":[],"source":["%matplotlib inline\n","plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n","plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knTkc-oMNxGN"},"outputs":[],"source":["plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n","plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracies')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0RJdVtQOD4x"},"outputs":[],"source":["def compute_confusion_matrix(predictions: List[int], labels:List[int]) -\u003e Tuple[int]:\n","    \"\"\"\n","    Compute the confusion matrix.\n","    Arguments:\n","        predictions: list of model predictions (0 for cats or 1 for dogs)\n","        labels: list of ground truth labels (0 or 1)\n","    \"\"\"\n","    correct_cats = 0\n","    correct_dogs = 0\n","    wrong_cats = 0\n","    wrong_dogs = 0\n","    for (p,l) in zip(predictions, labels):\n","        if p == 0 and l == 0:\n","            correct_cats += 1\n","        elif p == 0 and l == 1:\n","            wrong_cats += 1\n","        elif p == 1 and l == 1:\n","            correct_dogs += 1\n","        else:\n","            wrong_dogs += 1\n","\n","    return (correct_cats, wrong_cats, correct_dogs, wrong_dogs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JW6GMLpuPJvA"},"outputs":[],"source":["def eval_epoch(model, val_dataloader, loss_crt, device):\n","    \"\"\"\n","    model: Model object\n","    val_dataloader: DataLoader over the validation dataset\n","    loss_crt: loss function object\n","    device: torch.device('cpu) or torch.device('cuda')\n","\n","    The function returns:\n","     - the epoch validation loss, which is an average over the individual batch\n","       losses\n","     - the predictions made by the model\n","     - the labels\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0.0\n","    num_batches = len(val_dataloader)\n","    predictions = []\n","    labels = []\n","    with torch.no_grad():\n","        for batch_content, batch_labels in tqdm(val_dataloader):\n","            batch_labels = batch_labels \\\n","                  .type(torch.FloatTensor) \\\n","                  .reshape((batch_labels.shape[0], 1))\n","            batch_labels = batch_labels.to(device)\n","\n","            mask = batch_content['attention_mask'].to(device)\n","            input_id = batch_content['input_ids'].squeeze(1).to(device)\n","            # batch_size x 10\n","            # feedforward\n","            out = model(input_id)\n","            probabilities = torch.sigmoid(out.squeeze())  # Apply sigmoid activation\n","            batch_predictions = (probabilities \u003e= 0.5).long()\n","\n","            predictions += batch_predictions.squeeze().tolist()\n","            labels += batch_labels.squeeze().tolist()\n","\n","            loss = loss_crt(out, batch_labels)\n","            loss_scalar = loss.item()\n","\n","            epoch_loss += loss_scalar\n","\n","    epoch_loss = epoch_loss/num_batches\n","\n","    return epoch_loss, predictions, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AapZlSVMPL-D"},"outputs":[],"source":["fin_loss, preds, labels = eval_epoch(\n","    model, val_dataloader, loss_crt, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAhsT4VHurIN"},"outputs":[],"source":["len(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFxWDSv6N6gI"},"outputs":[],"source":["fin_loss, preds, labels = eval_epoch(\n","    model, val_dataloader, loss_crt, device)\n","\n","\n","correct_cats, wrong_cats, correct_dogs, wrong_dogs = compute_confusion_matrix(\n","    preds,\n","    labels\n",")\n","cf_matrix = [\n","    [correct_cats, wrong_cats],\n","    [wrong_dogs, correct_dogs]\n","]\n","import seaborn as sns\n","\n","ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='d')\n","\n","ax.set_title('Confusion Matrix\\n\\n');\n","ax.set_xlabel('Actual Values')\n","ax.set_ylabel('Predicted Values ');\n","\n","## Ticket labels - List must be in alphabetical order\n","ax.xaxis.set_ticklabels(['0','1'])\n","ax.yaxis.set_ticklabels(['0','1'])\n","\n","## Display the visualization of the Confusion Matrix.\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwtqQ5GSqlcH"},"outputs":[],"source":["len(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDtbWdBxb2gG"},"outputs":[],"source":["model1 = model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbanWdtzigiL"},"outputs":[],"source":["fin_loss, val_predictions, val_labels = eval_epoch(\n","    model1, val_dataloader, loss_crt, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W6rxIwydF8i"},"outputs":[],"source":["model2 = model ##### overfit nebun, 512, 1024, 1025 ---//----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX2ix_STeNHL"},"outputs":[],"source":["model3 = model ###### 12 epoci, 256, 256, 256,      ...... , 20 376...."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CF_3VmCjz0Hi"},"outputs":[],"source":["print(torch.bincount(y_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOd_5oetFa6k"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, r2_score\n","\n","print(f1_score(labels, preds))\n","\n","print(recall_score(labels, preds))\n","\n","print(precision_score(labels, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBHzpARaFbWL"},"outputs":[],"source":["test_data = pd.read_csv('/content/drive/MyDrive/NLP3/test.csv')\n","test_data['class'] = [True for _ in range(len(test_data['title']))]\n","print(test_data.head())\n","# print(df_test.head())\n","\n","\n","print(len(test_data))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFtgqcFeFes7"},"outputs":[],"source":["from sklearn.metrics import balanced_accuracy_score\n","\n","def balanced_accuracy_evaluate(model, test_data):\n","    final_ans = []\n","\n","\n","    fin_loss, preds, labels = eval_epoch(\n","    model, test_dataloader, loss_crt, device)\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR0W7TwOQcCY"},"outputs":[],"source":["test_data['title'] = test_data['title'].apply(lambda text: preprocess(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_u3SvUDFhDc"},"outputs":[],"source":["test = Dataset(test_data)\n","\n","test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n","\n","predict, labels = balanced_accuracy_evaluate(model, test)\n","\n","print(len(predict))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXP3JL4JFmEL"},"outputs":[],"source":["def code_to_text(code):\n","  labels = [False, True]\n","  return labels[code]\n","\n","\n","\n","\n","y_pred = []\n","output = predict\n","for i in range(len(output)):\n","  try:\n","    item0, item1 = output[i]\n","    y_pred.extend([code_to_text(item0), code_to_text(item1)])\n","  except:\n","    item0 = output[i]\n","    y_pred.append(code_to_text(item0))\n","predict = y_pred\n","\n","\n","i = range(len(predict))\n","dict = {'id':i, 'class':predict}\n","\n","final = pd.DataFrame(dict)\n","\n","fc = final.columns[0]\n","final = final.drop([fc], axis=1)\n","# final.columns = ['Id', 'Label']\n","final.to_csv('final.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4dpPfcpGGnE"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}